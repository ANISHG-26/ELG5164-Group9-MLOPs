{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\r\n",
        "# Imports\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import os\r\n",
        "import zipfile\r\n",
        "import datetime\r\n",
        "from io import BytesIO\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing import image as keras_image_preprocessing\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "# Constants for Azure\r\n",
        "STORAGE_CONNECTION_STRING = os.getenv('CONNECTION_STRING')\r\n",
        "CONTAINER_NAME = os.getenv('CONTAINER_NAME')\r\n",
        "BLOB_DATA_FILE = 'FilePath/garbage_dataset_importfile.csv'\r\n",
        "\r\n",
        "# Constants for ML\r\n",
        "IMAGE_WIDTH = 224    \r\n",
        "IMAGE_HEIGHT = 224\r\n",
        "IMAGE_CHANNELS = 3\r\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\r\n",
        "IMAGE_SHAPE = (224,224,3)\r\n",
        "NUM_CLASSES = 6\r\n",
        "categories = {'paper': 0,'cardboard': 1,'plastic': 2,'metal': 3,'trash': 4,'glass': 5}\r\n",
        "BATCH_SIZE = 16\r\n",
        "EPOCHS = 15\r\n",
        "MODEL_DIR = \"models/\"\r\n",
        "\r\n",
        "# Custom utilities\r\n",
        "def zipfolder(foldername, filename, includeEmptyDir=True):   \r\n",
        "    empty_dirs = []  \r\n",
        "    zip = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)  \r\n",
        "    for root, dirs, files in os.walk(foldername):  \r\n",
        "        empty_dirs.extend([dir for dir in dirs if os.listdir(os.path.join(root, dir)) == []])  \r\n",
        "        for name in files:  \r\n",
        "            zip.write(os.path.join(root ,name))  \r\n",
        "        if includeEmptyDir:  \r\n",
        "            for dir in empty_dirs:  \r\n",
        "                zif = zipfile.ZipInfo(os.path.join(root, dir) + \"/\")  \r\n",
        "                zip.writestr(zif, \"\")  \r\n",
        "        empty_dirs = []  \r\n",
        "    zip.close()\r\n",
        "    \r\n",
        "# Custom utilities for model training\r\n",
        "def custom_azure_dataframe_iterator(df, batch_size, container_client):\r\n",
        "    num_classes = len(df['label'].unique())\r\n",
        "    while True:\r\n",
        "        # iterate over batches of the dataframe\r\n",
        "        for i in range(0, len(df), batch_size):\r\n",
        "            # get the batch of file paths and labels\r\n",
        "            batch_df = df.iloc[i:i+batch_size]\r\n",
        "            batch_paths = batch_df['image_azure_location'].values\r\n",
        "            batch_labels = batch_df['label'].values\r\n",
        "            # load and preprocess the images in the batch\r\n",
        "            batch_images = []\r\n",
        "            for azure_blob_path in batch_paths:\r\n",
        "                path = \"/\".join(azure_blob_path.split('/')[3:])\r\n",
        "                blob_client = container_client.get_blob_client(blob=path)\r\n",
        "                blob_data = blob_client.download_blob().readall()\r\n",
        "                azure_image = Image.open(BytesIO(blob_data)).convert('RGB')\r\n",
        "                azure_image = azure_image.resize((224, 224))\r\n",
        "                azure_image = np.array(azure_image).astype('float32') / 255.0\r\n",
        "                batch_images.append(azure_image)\r\n",
        "            # Yield the preprocessed images and one-hot encoded labels as a batch\r\n",
        "            yield np.array(batch_images), to_categorical(batch_labels, num_classes=num_classes)\r\n",
        "\r\n",
        "# Azure Set Up\r\n",
        "def get_azure_blob_client():\r\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(STORAGE_CONNECTION_STRING)\r\n",
        "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=BLOB_DATA_FILE)\r\n",
        "    return blob_client\r\n",
        "\r\n",
        "def get_azure_container_client():\r\n",
        "  blob_service_client = BlobServiceClient.from_connection_string(STORAGE_CONNECTION_STRING)\r\n",
        "  container_client = blob_service_client.get_container_client(CONTAINER_NAME)\r\n",
        "  return container_client\r\n",
        "\r\n",
        "# Data preparation for custom training\r\n",
        "def data_prep():\r\n",
        "    blob_client = get_azure_blob_client()\r\n",
        "    \r\n",
        "    # Download the blob to a local file\r\n",
        "    with open(\"data.csv\", \"wb\") as my_blob:\r\n",
        "      download_stream = blob_client.download_blob()\r\n",
        "      my_blob.write(download_stream.readall())\r\n",
        "    \r\n",
        "    df = pd.read_csv(\"data.csv\",header=None)\r\n",
        "    df.columns = ['image_azure_location', 'label']\r\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\r\n",
        "    \r\n",
        "    # Change the categories from numbers to names\r\n",
        "    df[\"label\"] = df[\"label\"].map(categories).astype(str)\r\n",
        "    \r\n",
        "    # We first split the data into two sets and then split the validate_df to two sets\r\n",
        "    train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\r\n",
        "    validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\r\n",
        "\r\n",
        "    train_df = train_df.reset_index(drop=True)\r\n",
        "    validate_df = validate_df.reset_index(drop=True)\r\n",
        "    test_df = test_df.reset_index(drop=True)\r\n",
        "    \r\n",
        "    total_train = train_df.shape[0]\r\n",
        "    total_test = test_df.shape[0]\r\n",
        "    total_validate = validate_df.shape[0]\r\n",
        "    \r\n",
        "    print(\"#################### DATA METRICS ####################\")\r\n",
        "    print('train size = ', total_train, 'validate size = ', total_validate, 'test size = ', total_test)\r\n",
        "    \r\n",
        "    return (train_df,validate_df,total_train,total_validate)\r\n",
        "\r\n",
        "# Building Tensorflow Model\r\n",
        "def get_model(image_shape,num_classes):\r\n",
        "    \r\n",
        "    model = keras.Sequential([\r\n",
        "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=image_shape),\r\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\r\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n",
        "        keras.layers.Flatten(),\r\n",
        "        keras.layers.Dense(128, activation='relu'),\r\n",
        "        keras.layers.Dropout(0.5),\r\n",
        "        keras.layers.Dense(num_classes, activation='softmax')\r\n",
        "    ])\r\n",
        "    \r\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "    \r\n",
        "    return model\r\n",
        "    \r\n",
        "            \r\n",
        "def train():\r\n",
        "    print(\"#################### TRAINING STARTS ####################\")\r\n",
        "    \r\n",
        "    # Getting the data\r\n",
        "    train_df, validate_df, total_train, total_validate = data_prep()\r\n",
        "    \r\n",
        "    # Getting the model\r\n",
        "    model = get_model(IMAGE_SHAPE,NUM_CLASSES)\r\n",
        "    \r\n",
        "    # Create model save directory\r\n",
        "    model_save_path = os.path.join(os.getcwd(), MODEL_DIR)\r\n",
        "    os.mkdir(model_save_path) \r\n",
        "    print(\"Directory '% s' created\" % MODEL_DIR)\r\n",
        "\r\n",
        "    azure_container = get_azure_container_client()\r\n",
        "\r\n",
        "    train_generator = custom_azure_dataframe_iterator(train_df,BATCH_SIZE,azure_container)\r\n",
        "    validation_generator = custom_azure_dataframe_iterator(validate_df,BATCH_SIZE,azure_container)\r\n",
        "    \r\n",
        "    # Model Training\r\n",
        "    history = model.fit_generator(\r\n",
        "                generator=train_generator, \r\n",
        "                epochs=EPOCHS,\r\n",
        "                validation_data=validation_generator,\r\n",
        "                validation_steps=total_validate//BATCH_SIZE,\r\n",
        "                steps_per_epoch=total_train//BATCH_SIZE,\r\n",
        "                #callbacks=callbacks\r\n",
        "            )\r\n",
        "    \r\n",
        "    CONCRETE_INPUT = \"numpy_inputs\"\r\n",
        "    \r\n",
        "    # Tensorflow serving utilities\r\n",
        "    def _preprocess(bytes_input):\r\n",
        "        decoded = tf.io.decode_jpeg(bytes_input, channels=3)\r\n",
        "        decoded = tf.image.convert_image_dtype(decoded, tf.float32)\r\n",
        "        resized = tf.image.resize(decoded, size=(224, 224))\r\n",
        "        return resized\r\n",
        "\r\n",
        "\r\n",
        "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\r\n",
        "    def preprocess_fn(bytes_inputs):\r\n",
        "        decoded_images = tf.map_fn(\r\n",
        "            _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\r\n",
        "        )\r\n",
        "        return {\r\n",
        "            CONCRETE_INPUT: decoded_images\r\n",
        "        }  # User needs to make sure the key matches model's input\r\n",
        "\r\n",
        "\r\n",
        "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\r\n",
        "    def serving_fn(bytes_inputs):\r\n",
        "        images = preprocess_fn(bytes_inputs)\r\n",
        "        prob = m_call(**images)\r\n",
        "        return prob\r\n",
        "\r\n",
        "\r\n",
        "    m_call = tf.function(model.call).get_concrete_function(\r\n",
        "        [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\r\n",
        "    )\r\n",
        "\r\n",
        "    tf.saved_model.save(model, model_save_path, signatures={\"serving_default\": serving_fn})\r\n",
        "    \r\n",
        "    ct = datetime.datetime.now()\r\n",
        "    ct_string = ct.strftime('%Y-%m-%d %H:%M:%S')\r\n",
        "    \r\n",
        "    zipfile_name = 'models'+ct_string+'.zip'\r\n",
        "    zipfolder(MODEL_DIR,zipfile_name)\r\n",
        "\r\n",
        "    # Create a new directory\r\n",
        "    model_local_directory = \"Trained_models\"\r\n",
        "    os.mkdir(model_local_directory)\r\n",
        "\r\n",
        "    # Move a file into the new directory\r\n",
        "    file_to_move = zipfile_name\r\n",
        "    os.rename(file_to_move, os.path.join(model_local_directory, file_to_move))\r\n",
        "    \r\n",
        "    azure_blob_upload_path = \"Trained_models/\" + zipfile_name\r\n",
        "    upload_blob(azure_blob_upload_path)  \r\n",
        "    \r\n",
        "def upload_blob(source_file_name):\r\n",
        "    azure_container = get_azure_container_client() \r\n",
        "    blob_client = azure_container.get_blob_client(source_file_name)\r\n",
        "    with open(source_file_name, \"rb\") as data:\r\n",
        "      blob_client.upload_blob(data, overwrite=False)\r\n",
        "\r\n",
        "    print(f'{source_file_name} has been uploaded to {azure_container}.')\r\n",
        "    \r\n",
        "if __name__ == '__main__':\r\n",
        "    print('main')\r\n",
        "    train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting train.py\n"
        }
      ],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\r\n",
        "azure-core==1.26.3\r\n",
        "azure-storage-blob==12.15.0\r\n",
        "scikit-learn==1.2.2\r\n",
        "pandas==1.4.4\r\n",
        "numpy==1.22.4\r\n",
        "Pillow==8.4.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting requirements.txt\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\r\n",
        "FROM tensorflow/tensorflow:2.11.0\r\n",
        "WORKDIR /root\r\n",
        "\r\n",
        "ENV CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=garbagenetclas4048036826;AccountKey=EqBkL/lJUkc01dqCxs9FOrQ1qlqZBdjLYMg2kuhhs1Cx945LjUpDyAtReCNkVUThBeyF6Qnmw6re+AStO0YfxA==;EndpointSuffix=core.windows.net\"\r\n",
        "ENV CONTAINER_NAME=\"azureml-blobstore-afaa340e-8029-4073-bfbd-01de34e364bd\"\r\n",
        "\r\n",
        "COPY requirements.txt ./requirements.txt\r\n",
        "COPY train.py ./train.py\r\n",
        "\r\n",
        "RUN pip3 install -r requirements.txt\r\n",
        "\r\n",
        "ENTRYPOINT [\"python3\", \"train.py\"]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting Dockerfile\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile garbage-train-image-build-push.yml\r\n",
        "trigger:\r\n",
        "- main\r\n",
        "\r\n",
        "pool:\r\n",
        "  name: Default\r\n",
        "\r\n",
        "jobs:\r\n",
        "- job:\r\n",
        "  displayName: \"Model Training\"\r\n",
        "  \r\n",
        "  steps:\r\n",
        "  \r\n",
        "  - task: Docker@0\r\n",
        "    displayName: 'Build an image'\r\n",
        "    inputs:\r\n",
        "      azureSubscription: 'Azure subscription 1(e6d4c51e-0d82-4960-829a-196186580859)'\r\n",
        "      azureContainerRegistry: '{\"loginServer\":\"garbagenettrainer.azurecr.io\", \"id\" : \"/subscriptions/e6d4c51e-0d82-4960-829a-196186580859/resourceGroups/ELG5164-Garbagenet/providers/Microsoft.ContainerRegistry/registries/GarbagenetTrainer\"}'\r\n",
        "      imageName: 'garbagenet-trainer'\r\n",
        "      includeLatestTag: true\r\n",
        "      \r\n",
        "  - script: |\r\n",
        "      docker run garbagenettrainer.azurecr.io/garbagenet-trainer\r\n",
        "      \r\n",
        "  - task: Docker@0\r\n",
        "    displayName: 'Push an image'\r\n",
        "    inputs:\r\n",
        "      azureSubscription: 'Azure subscription 1(e6d4c51e-0d82-4960-829a-196186580859)'\r\n",
        "      azureContainerRegistry: '{\"loginServer\":\"garbagenettrainer.azurecr.io\", \"id\" : \"/subscriptions/e6d4c51e-0d82-4960-829a-196186580859/resourceGroups/ELG5164-Garbagenet/providers/Microsoft.ContainerRegistry/registries/GarbagenetTrainer\"}'\r\n",
        "      action: 'Push an image'\r\n",
        "      imageName: 'garbagenet-trainer'\r\n",
        "      includeLatestTag: true"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting garbage-train-image-build-push.yml\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Dockerfile garbage-train-image-build-push.yml train.py\r\n",
        "!git commit -m \"Running Training Job\"\r\n",
        "!git push -u --all origin"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[master 37feea1] Running Training Job\n 1 file changed, 1 insertion(+), 1 deletion(-)\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 300 bytes | 10.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\nTo https://github.com/ANISHG-26/ELG5164-AzureML-CT.git\n   919422c..37feea1  master -> master\nbranch 'master' set up to track 'origin/master'.\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!az extension add --name azure-devops\r\n",
        "!az devops configure --defaults organization=https://dev.azure.com/varunvora24/ project=ELG-5164-Garbagenet-MLOPs\r\n",
        "!az login\r\n",
        "!az pipelines run --name \"Garbagenet-Trainer-Build-Push\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[93mExtension 'azure-devops' 0.26.0 is already installed.\u001b[0m\n\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code RJQ96QPC8 to authenticate.\u001b[0m\n[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"0db441e6-b9ba-4da4-96e2-097c08885f09\",\n    \"id\": \"e6d4c51e-0d82-4960-829a-196186580859\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Azure subscription 1\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"0db441e6-b9ba-4da4-96e2-097c08885f09\",\n    \"user\": {\n      \"name\": \"varunvora24@outlook.com\",\n      \"type\": \"user\"\n    }\n  }\n]\n{\n  \"appendCommitMessageToRunName\": true,\n  \"buildNumber\": \"20230401.8\",\n  \"buildNumberRevision\": 8,\n  \"controller\": null,\n  \"definition\": {\n    \"createdDate\": null,\n    \"drafts\": [],\n    \"id\": 3,\n    \"name\": \"Garbagenet-Trainer-Build-Push\",\n    \"path\": \"\\\\\",\n    \"project\": {\n      \"abbreviation\": null,\n      \"defaultTeamImageUrl\": null,\n      \"description\": null,\n      \"id\": \"a6a0f737-c3d1-4d20-8130-abc24d23f73a\",\n      \"lastUpdateTime\": \"2023-03-31T16:16:43.96Z\",\n      \"name\": \"ELG-5164-Garbagenet-MLOPs\",\n      \"revision\": 33,\n      \"state\": \"wellFormed\",\n      \"url\": \"https://dev.azure.com/varunvora24/_apis/projects/a6a0f737-c3d1-4d20-8130-abc24d23f73a\",\n      \"visibility\": \"public\"\n    },\n    \"queueStatus\": \"enabled\",\n    \"revision\": 2,\n    \"type\": \"build\",\n    \"uri\": \"vstfs:///Build/Definition/3\",\n    \"url\": \"https://dev.azure.com/varunvora24/a6a0f737-c3d1-4d20-8130-abc24d23f73a/_apis/build/Definitions/3?revision=2\"\n  },\n  \"deleted\": null,\n  \"deletedBy\": null,\n  \"deletedDate\": null,\n  \"deletedReason\": null,\n  \"demands\": null,\n  \"finishTime\": null,\n  \"id\": 26,\n  \"keepForever\": false,\n  \"lastChangedBy\": {\n    \"descriptor\": \"msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"directoryAlias\": null,\n    \"displayName\": \"varunvora24@outlook.com\",\n    \"id\": \"48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\",\n    \"imageUrl\": \"https://dev.azure.com/varunvora24/_apis/GraphProfile/MemberAvatars/msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"inactive\": null,\n    \"isAadIdentity\": null,\n    \"isContainer\": null,\n    \"isDeletedInOrigin\": null,\n    \"profileUrl\": null,\n    \"uniqueName\": \"varunvora24@outlook.com\",\n    \"url\": \"https://spsprodcca1.vssps.visualstudio.com/Ac0b6e777-f1a9-4805-b7d1-b9f94da04958/_apis/Identities/48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\"\n  },\n  \"lastChangedDate\": \"2023-04-01T19:24:27.860000+00:00\",\n  \"logs\": {\n    \"id\": 0,\n    \"type\": \"Container\",\n    \"url\": \"https://dev.azure.com/varunvora24/a6a0f737-c3d1-4d20-8130-abc24d23f73a/_apis/build/builds/26/logs\"\n  },\n  \"orchestrationPlan\": {\n    \"orchestrationType\": null,\n    \"planId\": \"7a3c34cb-a02e-4c54-8324-3a0d3fb2d8a4\"\n  },\n  \"parameters\": null,\n  \"plans\": [\n    {\n      \"orchestrationType\": null,\n      \"planId\": \"7a3c34cb-a02e-4c54-8324-3a0d3fb2d8a4\"\n    }\n  ],\n  \"priority\": \"normal\",\n  \"project\": {\n    \"abbreviation\": null,\n    \"defaultTeamImageUrl\": null,\n    \"description\": null,\n    \"id\": \"a6a0f737-c3d1-4d20-8130-abc24d23f73a\",\n    \"lastUpdateTime\": \"2023-03-31T16:16:43.96Z\",\n    \"name\": \"ELG-5164-Garbagenet-MLOPs\",\n    \"revision\": 33,\n    \"state\": \"wellFormed\",\n    \"url\": \"https://dev.azure.com/varunvora24/_apis/projects/a6a0f737-c3d1-4d20-8130-abc24d23f73a\",\n    \"visibility\": \"public\"\n  },\n  \"properties\": {},\n  \"quality\": null,\n  \"queue\": {\n    \"id\": 18,\n    \"name\": \"Azure Pipelines\",\n    \"pool\": {\n      \"id\": 9,\n      \"isHosted\": true,\n      \"name\": \"Azure Pipelines\"\n    },\n    \"url\": null\n  },\n  \"queueOptions\": null,\n  \"queuePosition\": null,\n  \"queueTime\": \"2023-04-01T19:24:26.764917+00:00\",\n  \"reason\": \"manual\",\n  \"repository\": {\n    \"checkoutSubmodules\": false,\n    \"clean\": null,\n    \"defaultBranch\": null,\n    \"id\": \"ANISHG-26/ELG5164-AzureML-CT\",\n    \"name\": null,\n    \"properties\": null,\n    \"rootFolder\": null,\n    \"type\": \"GitHub\",\n    \"url\": null\n  },\n  \"requestedBy\": {\n    \"descriptor\": \"msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"directoryAlias\": null,\n    \"displayName\": \"varunvora24@outlook.com\",\n    \"id\": \"48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\",\n    \"imageUrl\": \"https://dev.azure.com/varunvora24/_apis/GraphProfile/MemberAvatars/msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"inactive\": null,\n    \"isAadIdentity\": null,\n    \"isContainer\": null,\n    \"isDeletedInOrigin\": null,\n    \"profileUrl\": null,\n    \"uniqueName\": \"varunvora24@outlook.com\",\n    \"url\": \"https://spsprodcca1.vssps.visualstudio.com/Ac0b6e777-f1a9-4805-b7d1-b9f94da04958/_apis/Identities/48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\"\n  },\n  \"requestedFor\": {\n    \"descriptor\": \"msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"directoryAlias\": null,\n    \"displayName\": \"varunvora24@outlook.com\",\n    \"id\": \"48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\",\n    \"imageUrl\": \"https://dev.azure.com/varunvora24/_apis/GraphProfile/MemberAvatars/msa.NDhiZjlkZGMtNTIxYS03ZTdjLWFlOTctNmQxOWQ3ZTQ4NDVl\",\n    \"inactive\": null,\n    \"isAadIdentity\": null,\n    \"isContainer\": null,\n    \"isDeletedInOrigin\": null,\n    \"profileUrl\": null,\n    \"uniqueName\": \"varunvora24@outlook.com\",\n    \"url\": \"https://spsprodcca1.vssps.visualstudio.com/Ac0b6e777-f1a9-4805-b7d1-b9f94da04958/_apis/Identities/48bf9ddc-521a-6e7c-ae97-6d19d7e4845e\"\n  },\n  \"result\": null,\n  \"retainedByRelease\": false,\n  \"sourceBranch\": \"refs/heads/master\",\n  \"sourceVersion\": \"37feea1298f4ad3a5b4a47f18d7a2a7b3bbbc369\",\n  \"startTime\": null,\n  \"status\": \"notStarted\",\n  \"tags\": [],\n  \"triggerInfo\": {},\n  \"triggeredByBuild\": null,\n  \"uri\": \"vstfs:///Build/Build/26\",\n  \"url\": \"https://dev.azure.com/varunvora24/a6a0f737-c3d1-4d20-8130-abc24d23f73a/_apis/build/Builds/26\",\n  \"validationResults\": []\n}\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Dockerfile garbage-train-image-build-push.yml train.py\r\n",
        "!git commit -m \"Running Training Job\"\r\n",
        "!git push -u --all origin"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}