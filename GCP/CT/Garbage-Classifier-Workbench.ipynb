{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdba82e6-10e6-46c7-88fa-fa3d19a7c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import zipfile\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image as keras_image_preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Constants for GCP\n",
    "BUCKET_NAME = \"cloud-ai-platform-54bd831b-8d3c-431d-acbc-4f155b670016\"\n",
    "GCS_DATA_FILE_PATH = \"gs://cloud-ai-platform-54bd831b-8d3c-431d-acbc-4f155b670016/FilePath/garbage_dataset_importfile.csv\"\n",
    "PROJECT_ID = \"beaming-team-376517\"\n",
    "KEY_PATH = 'beaming-team-376517-82a98807d500.json'\n",
    "\n",
    "# Constants for ML\n",
    "IMAGE_WIDTH = 224    \n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_SHAPE = (224,224,3)\n",
    "NUM_CLASSES = 6\n",
    "categories = {'paper': 0,'cardboard': 1,'plastic': 2,'metal': 3,'trash': 4,'glass': 5}\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MODEL_DIR = \"models/\"\n",
    "\n",
    "# Custom utilities\n",
    "def zipfolder(foldername, filename, includeEmptyDir=True):   \n",
    "    empty_dirs = []  \n",
    "    zip = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)  \n",
    "    for root, dirs, files in os.walk(foldername):  \n",
    "        empty_dirs.extend([dir for dir in dirs if os.listdir(os.path.join(root, dir)) == []])  \n",
    "        for name in files:  \n",
    "            zip.write(os.path.join(root ,name))  \n",
    "        if includeEmptyDir:  \n",
    "            for dir in empty_dirs:  \n",
    "                zif = zipfile.ZipInfo(os.path.join(root, dir) + \"/\")  \n",
    "                zip.writestr(zif, \"\")  \n",
    "        empty_dirs = []  \n",
    "    zip.close()\n",
    "    \n",
    "# Custom utilities for model training\n",
    "def custom_gcp_dataframe_iterator(df, batch_size, bucket):\n",
    "    num_classes = len(df['label'].unique())\n",
    "    while True:\n",
    "        # iterate over batches of the dataframe\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            # get the batch of file paths and labels\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            batch_paths = batch_df['image_gcp_location'].values\n",
    "            batch_labels = batch_df['label'].values\n",
    "            # load and preprocess the images in the batch\n",
    "            batch_images = []\n",
    "            for gcs_path in batch_paths:\n",
    "                path = \"/\".join(gcs_path.split('/')[3:])\n",
    "                blob = bucket.blob(path)\n",
    "                gcs_image_bytes = blob.download_as_bytes()\n",
    "                gcs_image = Image.open(BytesIO(gcs_image_bytes)).convert('RGB')\n",
    "                gcs_image = gcs_image.resize((224, 224))\n",
    "                gcs_image = np.array(gcs_image).astype('float32') / 255.0\n",
    "                batch_images.append(gcs_image)\n",
    "            # Yield the preprocessed images and one-hot encoded labels as a batch\n",
    "            yield np.array(batch_images), to_categorical(batch_labels, num_classes=num_classes)\n",
    "\n",
    "# GCP Set Up\n",
    "def get_gcp_bucket():\n",
    "    credentials = service_account.Credentials.from_service_account_file(KEY_PATH)\n",
    "    storage_client = storage.Client(project=PROJECT_ID,credentials=credentials)\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "\n",
    "# Data preparation for custom training\n",
    "def data_prep():\n",
    "    gcs_file_path = \"FilePath/garbage_dataset_importfile.csv\"\n",
    "    bucket = get_gcp_bucket()\n",
    "    blob = bucket.blob(gcs_file_path)\n",
    "    blob.download_to_filename('data.csv')\n",
    "    df = pd.read_csv(\"data.csv\",header=None)\n",
    "    df.columns = ['image_gcp_location', 'label']\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Change the categories from numbers to names\n",
    "    df[\"label\"] = df[\"label\"].map(categories).astype(str)\n",
    "    \n",
    "    # We first split the data into two sets and then split the validate_df to two sets\n",
    "    train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    validate_df = validate_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    total_train = train_df.shape[0]\n",
    "    total_test = test_df.shape[0]\n",
    "    total_validate = validate_df.shape[0]\n",
    "    \n",
    "    print(\"#################### DATA METRICS ####################\")\n",
    "    print('train size = ', total_train, 'validate size = ', total_validate, 'test size = ', total_test)\n",
    "    \n",
    "    return (train_df,validate_df,total_train,total_validate)\n",
    "\n",
    "# Building Tensorflow Model\n",
    "def get_model(image_shape,num_classes):\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=image_shape),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "            \n",
    "def train():\n",
    "    print(\"#################### TRAINING STARTS ####################\")\n",
    "    \n",
    "    # Getting the data\n",
    "    train_df, validate_df, total_train, total_validate = data_prep()\n",
    "    \n",
    "    # Getting the model\n",
    "    model = get_model(IMAGE_SHAPE,NUM_CLASSES)\n",
    "    \n",
    "    # Create model save directory\n",
    "    model_save_path = os.path.join(os.getcwd(), MODEL_DIR)\n",
    "    os.mkdir(model_save_path) \n",
    "    print(\"Directory '% s' created\" % MODEL_DIR)\n",
    "\n",
    "    bucket = get_gcp_bucket() \n",
    "\n",
    "    train_generator = custom_gcp_dataframe_iterator(train_df,BATCH_SIZE,bucket)\n",
    "    validation_generator = custom_gcp_dataframe_iterator(validate_df,BATCH_SIZE,bucket)\n",
    "    \n",
    "    # Model Training\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "                generator=train_generator, \n",
    "                epochs=EPOCHS,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=total_validate//BATCH_SIZE,\n",
    "                steps_per_epoch=total_train//BATCH_SIZE,\n",
    "                #callbacks=callbacks\n",
    "            )\n",
    "    \n",
    "    CONCRETE_INPUT = \"numpy_inputs\"\n",
    "    \n",
    "    # Tensorflow serving utilities\n",
    "    def _preprocess(bytes_input):\n",
    "        decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "        decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "        resized = tf.image.resize(decoded, size=(224, 224))\n",
    "        return resized\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def preprocess_fn(bytes_inputs):\n",
    "        decoded_images = tf.map_fn(\n",
    "            _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "        )\n",
    "        return {\n",
    "            CONCRETE_INPUT: decoded_images\n",
    "        }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def serving_fn(bytes_inputs):\n",
    "        images = preprocess_fn(bytes_inputs)\n",
    "        prob = m_call(**images)\n",
    "        return prob\n",
    "\n",
    "\n",
    "    m_call = tf.function(model.call).get_concrete_function(\n",
    "        [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    "    )\n",
    "\n",
    "    tf.saved_model.save(model, model_save_path, signatures={\"serving_default\": serving_fn})\n",
    "    \n",
    "    ct = datetime.datetime.now()\n",
    "    ct_string = ct.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    zipfile_name = 'models'+ct_string+'.zip'\n",
    "    zipfolder(MODEL_DIR,zipfile_name)\n",
    "    gcs_upload_path = \"Trained_models/\" + zipfile_name\n",
    "    upload_blob(zipfile_name, gcs_upload_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "def upload_blob(source_file_name, destination_blob_name):\n",
    "    bucket = get_gcp_bucket() \n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('main')\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e68ffd-55c2-4424-bf3f-40acda0ecea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tensorflow/tensorflow:2.11.0\n",
    "WORKDIR /root\n",
    "\n",
    "COPY requirements.txt ./requirements.txt\n",
    "COPY train.py ./train.py\n",
    "COPY beaming-team-376517-82a98807d500.json ./beaming-team-376517-82a98807d500.json\n",
    "\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f196ca0-b93a-4f37-8248-a6b5fa562d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# Build the container image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain', '.']\n",
    "# Push the container image to Container Registry\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain']\n",
    "images:\n",
    "- us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d55e738-0323-4f8c-954e-923627a8328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project beaming-team-376517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946823f5-43dc-40d2-8d1c-936e1077b7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 8623 file(s) totalling 47.8 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2023.03.27/19.26.13.329858.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://beaming-team-376517_cloudbuild/source/1679945173.93829-21d5f812240e419989f200dbc47634d1.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/beaming-team-376517/locations/global/builds/60ca0f9e-c075-4e89-abae-4d21d1d5cf19].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/60ca0f9e-c075-4e89-abae-4d21d1d5cf19?project=283526927149 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"60ca0f9e-c075-4e89-abae-4d21d1d5cf19\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://beaming-team-376517_cloudbuild/source/1679945173.93829-21d5f812240e419989f200dbc47634d1.tgz#1679945204127887\n",
      "Copying gs://beaming-team-376517_cloudbuild/source/1679945173.93829-21d5f812240e419989f200dbc47634d1.tgz#1679945204127887...\n",
      "/ [1 files][ 13.6 MiB/ 13.6 MiB]                                                \n",
      "Operation completed over 1 objects/13.6 MiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  58.87MB\n",
      "Step #0: Step 1/7 : FROM tensorflow/tensorflow:2.11.0\n",
      "Step #0: 2.11.0: Pulling from tensorflow/tensorflow\n",
      "Step #0: eaead16dc43b: Pulling fs layer\n",
      "Step #0: 83bb66f4018d: Pulling fs layer\n",
      "Step #0: a9d243755566: Pulling fs layer\n",
      "Step #0: 38d8f03945ed: Pulling fs layer\n",
      "Step #0: 0e62e78ef96b: Pulling fs layer\n",
      "Step #0: 311604e9ab28: Pulling fs layer\n",
      "Step #0: 584c5149ce07: Pulling fs layer\n",
      "Step #0: 3b5c5b94152b: Pulling fs layer\n",
      "Step #0: 0e62e78ef96b: Waiting\n",
      "Step #0: 311604e9ab28: Waiting\n",
      "Step #0: 584c5149ce07: Waiting\n",
      "Step #0: 3b5c5b94152b: Waiting\n",
      "Step #0: 38d8f03945ed: Waiting\n",
      "Step #0: eaead16dc43b: Verifying Checksum\n",
      "Step #0: eaead16dc43b: Download complete\n",
      "Step #0: 83bb66f4018d: Verifying Checksum\n",
      "Step #0: 83bb66f4018d: Download complete\n",
      "Step #0: 38d8f03945ed: Verifying Checksum\n",
      "Step #0: 38d8f03945ed: Download complete\n",
      "Step #0: 0e62e78ef96b: Verifying Checksum\n",
      "Step #0: 0e62e78ef96b: Download complete\n",
      "Step #0: 584c5149ce07: Verifying Checksum\n",
      "Step #0: 584c5149ce07: Download complete\n",
      "Step #0: 3b5c5b94152b: Verifying Checksum\n",
      "Step #0: 3b5c5b94152b: Download complete\n",
      "Step #0: a9d243755566: Verifying Checksum\n",
      "Step #0: a9d243755566: Download complete\n",
      "Step #0: eaead16dc43b: Pull complete\n",
      "Step #0: 311604e9ab28: Verifying Checksum\n",
      "Step #0: 311604e9ab28: Download complete\n",
      "Step #0: 83bb66f4018d: Pull complete\n",
      "Step #0: a9d243755566: Pull complete\n",
      "Step #0: 38d8f03945ed: Pull complete\n",
      "Step #0: 0e62e78ef96b: Pull complete\n",
      "Step #0: 311604e9ab28: Pull complete\n",
      "Step #0: 584c5149ce07: Pull complete\n",
      "Step #0: 3b5c5b94152b: Pull complete\n",
      "Step #0: Digest: sha256:eea5989852623037f354c49404b66761467516b79ab7af26e643b5ac7382c53f\n",
      "Step #0: Status: Downloaded newer image for tensorflow/tensorflow:2.11.0\n",
      "Step #0:  ---> 82f1344ecd48\n",
      "Step #0: Step 2/7 : WORKDIR /root\n",
      "Step #0:  ---> Running in 80bc96123f58\n",
      "Step #0: Removing intermediate container 80bc96123f58\n",
      "Step #0:  ---> 0324893c7fb3\n",
      "Step #0: Step 3/7 : COPY requirements.txt ./requirements.txt\n",
      "Step #0:  ---> ad06bd9f031c\n",
      "Step #0: Step 4/7 : COPY train.py ./train.py\n",
      "Step #0:  ---> 647541e201f4\n",
      "Step #0: Step 5/7 : COPY beaming-team-376517-82a98807d500.json ./beaming-team-376517-82a98807d500.json\n",
      "Step #0:  ---> 4dcfda93f1de\n",
      "Step #0: Step 6/7 : RUN pip3 install -r requirements.txt\n",
      "Step #0:  ---> Running in 1823b344dd1b\n",
      "Step #0: Collecting google-cloud-storage==2.7.0\n",
      "Step #0:   Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "Step #0: Collecting scikit-learn==1.2.2\n",
      "Step #0:   Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "Step #0: Collecting pandas==1.4.4\n",
      "Step #0:   Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Step #0: Collecting numpy==1.22.4\n",
      "Step #0:   Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "Step #0: Collecting Pillow==8.4.0\n",
      "Step #0:   Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Step #0: Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (0.4.6)\n",
      "Step #0: Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (2.14.1)\n",
      "Step #0: Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "Step #0:   Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Step #0: Collecting google-resumable-media>=2.3.2\n",
      "Step #0:   Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Step #0: Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "Step #0:   Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Step #0: Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Step #0: Collecting scipy>=1.3.2\n",
      "Step #0:   Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Step #0: Collecting threadpoolctl>=2.0.0\n",
      "Step #0:   Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting joblib>=1.1.1\n",
      "Step #0:   Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Step #0: Collecting pytz>=2020.1\n",
      "Step #0:   Downloading pytz-2023.2-py2.py3-none-any.whl (502 kB)\n",
      "Step #0: Collecting python-dateutil>=2.8.1\n",
      "Step #0:   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0: Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib==0.4.6->-r requirements.txt (line 6)) (1.3.1)\n",
      "Step #0: Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (4.9)\n",
      "Step #0: Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (5.2.0)\n",
      "Step #0: Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (0.2.8)\n",
      "Step #0: Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (3.19.6)\n",
      "Step #0: Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "Step #0: Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (1.26.12)\n",
      "Step #0: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (2022.9.24)\n",
      "Step #0: Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "Step #0: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (3.4)\n",
      "Step #0: Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib==0.4.6->-r requirements.txt (line 6)) (3.2.2)\n",
      "Step #0: Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.7.0->-r requirements.txt (line 1)) (0.4.8)\n",
      "Step #0: Installing collected packages: googleapis-common-protos, google-api-core, google-cloud-core, google-crc32c, google-resumable-media, google-cloud-storage, numpy, scipy, threadpoolctl, joblib, scikit-learn, pytz, python-dateutil, pandas, Pillow\n",
      "Step #0:   Attempting uninstall: numpy\n",
      "Step #0:     Found existing installation: numpy 1.23.4\n",
      "Step #0:     Uninstalling numpy-1.23.4:\n",
      "Step #0:       Successfully uninstalled numpy-1.23.4\n",
      "Step #0: Successfully installed Pillow-8.4.0 google-api-core-2.11.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.59.0 joblib-1.2.0 numpy-1.22.4 pandas-1.4.4 python-dateutil-2.8.2 pytz-2023.2 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "Step #0: \u001b[91mWARNING: You are using pip version 20.2.4; however, version 23.0.1 is available.\n",
      "Step #0: You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "Step #0: \u001b[0mRemoving intermediate container 1823b344dd1b\n",
      "Step #0:  ---> 30b95f76c4fe\n",
      "Step #0: Step 7/7 : ENTRYPOINT [\"python3\", \"train.py\"]\n",
      "Step #0:  ---> Running in 1a4d8a001272\n",
      "Step #0: Removing intermediate container 1a4d8a001272\n",
      "Step #0:  ---> 11bb3b936d6c\n",
      "Step #0: Successfully built 11bb3b936d6c\n",
      "Step #0: Successfully tagged us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Using default tag: latest\n",
      "Step #1: The push refers to repository [us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain]\n",
      "Step #1: f762b2567e1e: Preparing\n",
      "Step #1: 76dff963c33a: Preparing\n",
      "Step #1: a3114535657f: Preparing\n",
      "Step #1: 8c1fbe07160a: Preparing\n",
      "Step #1: d25f7660e6c7: Preparing\n",
      "Step #1: 582a80795eda: Preparing\n",
      "Step #1: e552716cf5da: Preparing\n",
      "Step #1: cf3e28d7ced8: Preparing\n",
      "Step #1: 1c75a2767473: Preparing\n",
      "Step #1: 55d30f8f8b35: Preparing\n",
      "Step #1: c5429f2feab6: Preparing\n",
      "Step #1: f4462d5b2da2: Preparing\n",
      "Step #1: 582a80795eda: Waiting\n",
      "Step #1: e552716cf5da: Waiting\n",
      "Step #1: cf3e28d7ced8: Waiting\n",
      "Step #1: 1c75a2767473: Waiting\n",
      "Step #1: 55d30f8f8b35: Waiting\n",
      "Step #1: c5429f2feab6: Waiting\n",
      "Step #1: f4462d5b2da2: Waiting\n",
      "Step #1: d25f7660e6c7: Layer already exists\n",
      "Step #1: 582a80795eda: Layer already exists\n",
      "Step #1: e552716cf5da: Layer already exists\n",
      "Step #1: cf3e28d7ced8: Layer already exists\n",
      "Step #1: 1c75a2767473: Layer already exists\n",
      "Step #1: 76dff963c33a: Pushed\n",
      "Step #1: a3114535657f: Pushed\n",
      "Step #1: 8c1fbe07160a: Pushed\n",
      "Step #1: c5429f2feab6: Layer already exists\n",
      "Step #1: 55d30f8f8b35: Layer already exists\n",
      "Step #1: f4462d5b2da2: Layer already exists\n",
      "Step #1: f762b2567e1e: Pushed\n",
      "Step #1: latest: digest: sha256:a10ff8e598c77627317834829e61079cbe2944e59339562bc05dcc1be385c515 size: 2836\n",
      "Finished Step #1\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain\n",
      "The push refers to repository [us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain]\n",
      "f762b2567e1e: Preparing\n",
      "76dff963c33a: Preparing\n",
      "a3114535657f: Preparing\n",
      "8c1fbe07160a: Preparing\n",
      "d25f7660e6c7: Preparing\n",
      "582a80795eda: Preparing\n",
      "e552716cf5da: Preparing\n",
      "cf3e28d7ced8: Preparing\n",
      "1c75a2767473: Preparing\n",
      "55d30f8f8b35: Preparing\n",
      "c5429f2feab6: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "582a80795eda: Waiting\n",
      "e552716cf5da: Waiting\n",
      "cf3e28d7ced8: Waiting\n",
      "1c75a2767473: Waiting\n",
      "55d30f8f8b35: Waiting\n",
      "c5429f2feab6: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "f762b2567e1e: Layer already exists\n",
      "76dff963c33a: Layer already exists\n",
      "a3114535657f: Layer already exists\n",
      "d25f7660e6c7: Layer already exists\n",
      "8c1fbe07160a: Layer already exists\n",
      "e552716cf5da: Layer already exists\n",
      "582a80795eda: Layer already exists\n",
      "55d30f8f8b35: Layer already exists\n",
      "cf3e28d7ced8: Layer already exists\n",
      "1c75a2767473: Layer already exists\n",
      "c5429f2feab6: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "latest: digest: sha256:a10ff8e598c77627317834829e61079cbe2944e59339562bc05dcc1be385c515 size: 2836\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                                                    STATUS\n",
      "60ca0f9e-c075-4e89-abae-4d21d1d5cf19  2023-03-27T19:26:44+00:00  1M39S     gs://beaming-team-376517_cloudbuild/source/1679945173.93829-21d5f812240e419989f200dbc47634d1.tgz  us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dd907c-e414-4856-971e-3d7992529443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230327192827\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "JOB_NAME=f\"garbage_train_{TIMESTAMP}\"\n",
    "CUSTOM_CONTAINER_IMAGE_URI=\"us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain:latest\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "print(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5813b8-ef58-47f9-919f-1a638ca1fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "workerPoolSpecs:\n",
    "    machineSpec:\n",
    "        machineType: n1-standard-8\n",
    "    replicaCount: 1\n",
    "    containerSpec:\n",
    "        imageUri: us-central1-docker.pkg.dev/beaming-team-376517/garbagenet-trainer/garbagetrain:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21950988-60a9-4b07-9bdf-f7ada9a197f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/283526927149/locations/us-central1/customJobs/4155238747002109952] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/283526927149/locations/us-central1/customJobs/4155238747002109952\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/283526927149/locations/us-central1/customJobs/4155238747002109952\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --config=config.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
