{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d1edc10-c4aa-4089-a0c8-0d4bd8dc6fde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import zipfile\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image as keras_image_preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Constants for AWS\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = os.getenv('ACCESS_KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = os.getenv('SECRET_KEY')\n",
    "BUCKET_NAME = \"garbagenet-bucket-30032023\"\n",
    "S3_DATA_FILE_PATH = 'FilePath/garbage_dataset_importfile.csv'\n",
    "\n",
    "# Constants for ML\n",
    "IMAGE_WIDTH = 224    \n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_SHAPE = (224,224,3)\n",
    "NUM_CLASSES = 6\n",
    "categories = {'paper': 0,'cardboard': 1,'plastic': 2,'metal': 3,'trash': 4,'glass': 5}\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MODEL_DIR = \"models/\"\n",
    "\n",
    "# Custom utilities\n",
    "def zipfolder(foldername, filename, includeEmptyDir=True):   \n",
    "    empty_dirs = []  \n",
    "    zip = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)  \n",
    "    for root, dirs, files in os.walk(foldername):  \n",
    "        empty_dirs.extend([dir for dir in dirs if os.listdir(os.path.join(root, dir)) == []])  \n",
    "        for name in files:  \n",
    "            zip.write(os.path.join(root ,name))  \n",
    "        if includeEmptyDir:  \n",
    "            for dir in empty_dirs:  \n",
    "                zif = zipfile.ZipInfo(os.path.join(root, dir) + \"/\")  \n",
    "                zip.writestr(zif, \"\")  \n",
    "        empty_dirs = []  \n",
    "    zip.close()\n",
    "    \n",
    "# Custom utilities for model training\n",
    "def custom_aws_dataframe_iterator(df, batch_size, s3_client):\n",
    "    num_classes = len(df['label'].unique())\n",
    "    while True:\n",
    "        # iterate over batches of the dataframe\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            # get the batch of file paths and labels\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            batch_paths = batch_df['image_aws_location'].values\n",
    "            batch_labels = batch_df['label'].values\n",
    "            # load and preprocess the images in the batch\n",
    "            batch_images = []\n",
    "            for s3_object_path in batch_paths:\n",
    "                s3_key = s3_object_path.split('/', 3)[3]\n",
    "                response = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "                s3_image = Image.open(BytesIO(response['Body'].read())).convert('RGB')\n",
    "                s3_image = s3_image.resize((224, 224))\n",
    "                s3_image = np.array(s3_image).astype('float32') / 255.0\n",
    "                batch_images.append(s3_image)\n",
    "            # Yield the preprocessed images and one-hot encoded labels as a batch\n",
    "            yield np.array(batch_images), to_categorical(batch_labels, num_classes=num_classes)\n",
    "\n",
    "# AWS Set Up\n",
    "def get_aws_s3_client():\n",
    "  s3_client = boto3.client('s3')\n",
    "  return s3_client\n",
    "\n",
    "# Data preparation for custom training\n",
    "def data_prep():\n",
    "    s3_client = get_aws_s3_client()\n",
    "    \n",
    "    # Set the local file path to save the downloaded object in the current directory\n",
    "    local_data_file_path = os.path.join(os.getcwd(), 'data.csv')\n",
    "\n",
    "    # Download the object from the bucket to a local file\n",
    "    s3_client.download_file(BUCKET_NAME, S3_DATA_FILE_PATH, local_data_file_path)\n",
    "    \n",
    "    df = pd.read_csv(\"data.csv\",header=None)\n",
    "    df.columns = ['image_aws_location', 'label']\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Change the categories from numbers to names\n",
    "    df[\"label\"] = df[\"label\"].map(categories).astype(str)\n",
    "    \n",
    "    # We first split the data into two sets and then split the validate_df to two sets\n",
    "    train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    validate_df = validate_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    total_train = train_df.shape[0]\n",
    "    total_test = test_df.shape[0]\n",
    "    total_validate = validate_df.shape[0]\n",
    "    \n",
    "    print(\"#################### DATA METRICS ####################\")\n",
    "    print('train size = ', total_train, 'validate size = ', total_validate, 'test size = ', total_test)\n",
    "    \n",
    "    return (train_df,validate_df,total_train,total_validate)\n",
    "\n",
    "# Building Tensorflow Model\n",
    "def get_model(image_shape,num_classes):\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=image_shape),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "            \n",
    "def train():\n",
    "    print(\"#################### TRAINING STARTS ####################\")\n",
    "    \n",
    "    # Getting the data\n",
    "    train_df, validate_df, total_train, total_validate = data_prep()\n",
    "    \n",
    "    # Getting the model\n",
    "    model = get_model(IMAGE_SHAPE,NUM_CLASSES)\n",
    "    \n",
    "    # Create model save directory\n",
    "    model_save_path = os.path.join(os.getcwd(), MODEL_DIR)\n",
    "    os.mkdir(model_save_path) \n",
    "    print(\"Directory '% s' created\" % MODEL_DIR)\n",
    "\n",
    "    s3_client = get_aws_s3_client()\n",
    "\n",
    "    train_generator = custom_aws_dataframe_iterator(train_df,BATCH_SIZE,s3_client)\n",
    "    validation_generator = custom_aws_dataframe_iterator(validate_df,BATCH_SIZE,s3_client)\n",
    "    \n",
    "    # Model Training\n",
    "    history = model.fit_generator(\n",
    "                generator=train_generator, \n",
    "                epochs=EPOCHS,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=total_validate//BATCH_SIZE,\n",
    "                steps_per_epoch=total_train//BATCH_SIZE,\n",
    "                #callbacks=callbacks\n",
    "            )\n",
    "    \n",
    "    CONCRETE_INPUT = \"numpy_inputs\"\n",
    "    \n",
    "    # Tensorflow serving utilities\n",
    "    def _preprocess(bytes_input):\n",
    "        decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "        decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "        resized = tf.image.resize(decoded, size=(224, 224))\n",
    "        return resized\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def preprocess_fn(bytes_inputs):\n",
    "        decoded_images = tf.map_fn(\n",
    "            _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "        )\n",
    "        return {\n",
    "            CONCRETE_INPUT: decoded_images\n",
    "        }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def serving_fn(bytes_inputs):\n",
    "        images = preprocess_fn(bytes_inputs)\n",
    "        prob = m_call(**images)\n",
    "        return prob\n",
    "\n",
    "\n",
    "    m_call = tf.function(model.call).get_concrete_function(\n",
    "        [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    "    )\n",
    "\n",
    "    tf.saved_model.save(model, model_save_path, signatures={\"serving_default\": serving_fn})\n",
    "    \n",
    "    ct = datetime.datetime.now()\n",
    "    ct_string = ct.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    zipfile_name = 'models'+ct_string+'.zip'\n",
    "    zipfolder(MODEL_DIR,zipfile_name)\n",
    "\n",
    "    # Create a new directory\n",
    "    model_local_directory = \"Trained_models\"\n",
    "    os.mkdir(model_local_directory)\n",
    "\n",
    "    # Move a file into the new directory\n",
    "    file_to_move = zipfile_name\n",
    "    os.rename(file_to_move, os.path.join(model_local_directory, file_to_move))\n",
    "    \n",
    "    aws_s3_upload_path = \"Trained_models/\" + zipfile_name\n",
    "    upload_blob(aws_s3_upload_path)  \n",
    "    \n",
    "def upload_blob(upload_path):\n",
    "    s3_client = get_aws_s3_client()\n",
    "    s3_client.upload_file(upload_path, BUCKET_NAME, upload_path)\n",
    "\n",
    "    print(f'{upload_path} has been uploaded to {BUCKET_NAME}.')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('main')\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3a7f4a5-e215-42be-abbe-ca2f69174bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tensorflow/tensorflow:2.11.0\n",
    "WORKDIR /root\n",
    "\n",
    "ENV ACCESS_KEY=\"AKIA5F3ECBCQQ2SWFMG7\"\n",
    "ENV SECRET_KEY=\"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\"\n",
    "\n",
    "COPY requirements.txt ./requirements.txt\n",
    "COPY train.py ./train.py\n",
    "\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80a796ba-1926-4c03-882a-adb3885e8ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildspec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildspec.yaml\n",
    "version: 0.2\n",
    "phases:\n",
    "    pre_build:\n",
    "        commands:\n",
    "          - echo Logging in to Amazon ECR...\n",
    "          - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\n",
    "    build:\n",
    "        commands:\n",
    "          - echo Build started on `date`\n",
    "          - echo Building the Docker image...          \n",
    "          - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "          - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG      \n",
    "    post_build:\n",
    "        commands:\n",
    "          - echo Build completed on `date`\n",
    "          - echo Pushing the Docker image...\n",
    "          - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "env:\n",
    "    variables:\n",
    "        AWS_DEFAULT_REGION: \"us-east-2\"\n",
    "        AWS_ACCOUNT_ID: \"905911077025\"\n",
    "        IMAGE_REPO_NAME: \"garbagenet-train\"\n",
    "        IMAGE_TAG: \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d5112a4-4af5-4ba2-a4e8-028082a2b95b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 27e354c] Adding latest files\n",
      " Committer: EC2 Default User <ec2-user@ip-172-16-15-205.us-east-2.compute.internal>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly:\n",
      "\n",
      "    git config --global user.name \"Your Name\"\n",
      "    git config --global user.email you@example.com\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 331 bytes | 331.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Validating objects: 100%\u001b[K\n",
      "To https://git-codecommit.us-east-2.amazonaws.com/v1/repos/AmazonSageMaker-garbage-classifier-CT\n",
      "   4684f82..27e354c  master -> master\n",
      "branch 'master' set up to track 'origin/master'.\n"
     ]
    }
   ],
   "source": [
    "!git add Dockerfile buildspec.yaml train.py\n",
    "!git commit -m \"Adding latest files\"\n",
    "!git push -u --all origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47567b9c-e72b-453f-91a2-0d9f19a4ac34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"build\": {\n",
      "        \"id\": \"garbagenet-train-image-build-push:a3ceb0e9-6e19-4744-83d3-e07140fbbe40\",\n",
      "        \"arn\": \"arn:aws:codebuild:us-east-2:905911077025:build/garbagenet-train-image-build-push:a3ceb0e9-6e19-4744-83d3-e07140fbbe40\",\n",
      "        \"buildNumber\": 13,\n",
      "        \"startTime\": 1680156878.367,\n",
      "        \"currentPhase\": \"QUEUED\",\n",
      "        \"buildStatus\": \"IN_PROGRESS\",\n",
      "        \"sourceVersion\": \"refs/heads/master\",\n",
      "        \"projectName\": \"garbagenet-train-image-build-push\",\n",
      "        \"phases\": [\n",
      "            {\n",
      "                \"phaseType\": \"SUBMITTED\",\n",
      "                \"phaseStatus\": \"SUCCEEDED\",\n",
      "                \"startTime\": 1680156878.367,\n",
      "                \"endTime\": 1680156878.409,\n",
      "                \"durationInSeconds\": 0\n",
      "            },\n",
      "            {\n",
      "                \"phaseType\": \"QUEUED\",\n",
      "                \"startTime\": 1680156878.409\n",
      "            }\n",
      "        ],\n",
      "        \"source\": {\n",
      "            \"type\": \"CODECOMMIT\",\n",
      "            \"location\": \"https://git-codecommit.us-east-2.amazonaws.com/v1/repos/AmazonSageMaker-garbage-classifier-CT\",\n",
      "            \"gitCloneDepth\": 1,\n",
      "            \"gitSubmodulesConfig\": {\n",
      "                \"fetchSubmodules\": false\n",
      "            },\n",
      "            \"insecureSsl\": false\n",
      "        },\n",
      "        \"secondarySources\": [],\n",
      "        \"secondarySourceVersions\": [],\n",
      "        \"artifacts\": {\n",
      "            \"location\": \"\"\n",
      "        },\n",
      "        \"secondaryArtifacts\": [],\n",
      "        \"cache\": {\n",
      "            \"type\": \"NO_CACHE\"\n",
      "        },\n",
      "        \"environment\": {\n",
      "            \"type\": \"LINUX_CONTAINER\",\n",
      "            \"image\": \"aws/codebuild/standard:4.0\",\n",
      "            \"computeType\": \"BUILD_GENERAL1_SMALL\",\n",
      "            \"environmentVariables\": [],\n",
      "            \"privilegedMode\": true,\n",
      "            \"imagePullCredentialsType\": \"CODEBUILD\"\n",
      "        },\n",
      "        \"serviceRole\": \"arn:aws:iam::905911077025:role/service-role/codebuild-garbagenet-train-image-build-push-service-role\",\n",
      "        \"logs\": {\n",
      "            \"deepLink\": \"https://console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEvent:group=null;stream=null\",\n",
      "            \"cloudWatchLogsArn\": \"arn:aws:logs:us-east-2:905911077025:log-group:null:log-stream:null\",\n",
      "            \"cloudWatchLogs\": {\n",
      "                \"status\": \"ENABLED\"\n",
      "            },\n",
      "            \"s3Logs\": {\n",
      "                \"status\": \"DISABLED\",\n",
      "                \"encryptionDisabled\": false\n",
      "            }\n",
      "        },\n",
      "        \"timeoutInMinutes\": 45,\n",
      "        \"queuedTimeoutInMinutes\": 480,\n",
      "        \"buildComplete\": false,\n",
      "        \"initiator\": \"AmazonSageMaker-ExecutionRole-20230329T232165/SageMaker\",\n",
      "        \"encryptionKey\": \"arn:aws:kms:us-east-2:905911077025:alias/aws/s3\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws codebuild start-build --project-name garbagenet-train-image-build-push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdcfedac-cc0d-4cb9-8c1d-13dacdce58e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garbage-train-20230330064616\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "JOB_NAME=f\"garbage-train-{TIMESTAMP}\"\n",
    "\n",
    "print(JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7cfd38f-3b9d-4238-84d1-e75f3bb8c99a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TrainingJobArn\": \"arn:aws:sagemaker:us-east-2:905911077025:training-job/garbage-train-20230330064616\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker create-training-job \\\n",
    "    --training-job-name {JOB_NAME} \\\n",
    "    --algorithm-specification \"TrainingImage=905911077025.dkr.ecr.us-east-2.amazonaws.com/garbagenet-train:latest,TrainingInputMode=File\" \\\n",
    "    --output-data-config \"S3OutputPath=s3://garbagenet-bucket-30032023/Bypass/\" \\\n",
    "    --resource-config \"InstanceType=ml.m5.xlarge,InstanceCount=1,VolumeSizeInGB=1\" \\\n",
    "    --stopping-condition \"MaxRuntimeInSeconds=7200\" \\\n",
    "    --role-arn \"arn:aws:iam::905911077025:role/service-role/AmazonSageMaker-ExecutionRole-20230329T232165\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
